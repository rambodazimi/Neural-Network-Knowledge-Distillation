{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation on simple Convolutional Neural Networks (CNNs)"
      ],
      "metadata": {
        "id": "QvzaTKEZVCNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Imports"
      ],
      "metadata": {
        "id": "SBnzsFx7VQ-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "toyH3QbuGWvq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Loading\n",
        "Dataset: CIFAR-10"
      ],
      "metadata": {
        "id": "4NanCj8yVVeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process the dataset by converting it to Tensor as well as normalize using mean and std\n",
        "my_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Downloading the CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=my_transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=my_transform)\n",
        "\n",
        "# DataLoader ready to be used\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g0k85F3VZ-D",
        "outputId": "d62e752f-5b0f-4f34-a590-edc7f405b68f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definitions\n",
        "\n",
        "#### Teacher: A simple CNN with 4 Conv2D layers, 2 MaxPool2D, 1 Flatten, and 2 Linear layers\n",
        "\n",
        "#### Student: A simple CNN with 2 Conv2D layers, 2 MaxPool2D, 1 Flatten, and 2 Linear layers"
      ],
      "metadata": {
        "id": "vn0qCPuzWQpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TeacherModel(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(TeacherModel, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(32 * 8 * 8, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class StudentModel(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(StudentModel, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(16 * 8 * 8, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "cWA0KEpAWuts"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Individual Training\n",
        "First, we train each teacher and student model to calculate their accuracy without using knowledge distillation"
      ],
      "metadata": {
        "id": "Z8C72QIEZGn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_individual(model, train_loader, epochs, lr, device):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  model.train() # train mode\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device) # transfer to GPU\n",
        "\n",
        "      optimizer.zero_grad() # reset gradient\n",
        "      outputs = model(inputs) # generate response (probability distribution)\n",
        "      loss = criterion(outputs, labels) # calculate loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "  model.to(device) # transfer to GPU\n",
        "  model.eval() # eval mode\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad(): # no gradient update\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      outputs = model(inputs) # generate response (probability distribution)\n",
        "      _, predicted = torch.max(outputs.data, 1) # find the class with max probability\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "sBrtzbnCZSRM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Creation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "teacher_model = TeacherModel().to(device)\n",
        "student_model = StudentModel().to(device)\n",
        "\n",
        "total_params_deep = \"{:,}\".format(sum(p.numel() for p in teacher_model.parameters()))\n",
        "print(f\"Teacher Model parameters: {total_params_deep}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in student_model.parameters()))\n",
        "print(f\"Student Model parameters: {total_params_light}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRclta9Tc2YQ",
        "outputId": "c42549b7-d3f5-4696-ff14-51b22751f5ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Model parameters: 1,186,986\n",
            "Student Model parameters: 267,738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Teacher Model Training and Testing\n",
        "torch.manual_seed(42)\n",
        "epoch = 10\n",
        "lr = 0.001\n",
        "train_individual(teacher_model, train_loader, epoch, lr, device)\n",
        "test(teacher_model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ld-rNP-cLfB",
        "outputId": "ab2ed4a7-f735-4d1f-e9e2-6696ac8f57f2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.3853567789887529\n",
            "Epoch 2/10, Loss: 0.9018769300807162\n",
            "Epoch 3/10, Loss: 0.6984720657701078\n",
            "Epoch 4/10, Loss: 0.5410691216168806\n",
            "Epoch 5/10, Loss: 0.4124810284818225\n",
            "Epoch 6/10, Loss: 0.28769126050460064\n",
            "Epoch 7/10, Loss: 0.1918012785827717\n",
            "Epoch 8/10, Loss: 0.1417765292574835\n",
            "Epoch 9/10, Loss: 0.10655551437107498\n",
            "Epoch 10/10, Loss: 0.09177767301021177\n",
            "Test Accuracy: 74.43%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74.43"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Student Model Training and Testing\n",
        "torch.manual_seed(42)\n",
        "epoch = 10\n",
        "lr = 0.001\n",
        "train_individual(student_model, train_loader, epoch, lr, device)\n",
        "test(student_model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of9yPlTodvdq",
        "outputId": "3522c989-c5e0-4a79-85c3-6e775459d669"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.4606203031356988\n",
            "Epoch 2/10, Loss: 1.1367454313866012\n",
            "Epoch 3/10, Loss: 0.9992197162050116\n",
            "Epoch 4/10, Loss: 0.9032106946801286\n",
            "Epoch 5/10, Loss: 0.8217958803372005\n",
            "Epoch 6/10, Loss: 0.7537446095967841\n",
            "Epoch 7/10, Loss: 0.6937782188967976\n",
            "Epoch 8/10, Loss: 0.6249525134673204\n",
            "Epoch 9/10, Loss: 0.5771157086810188\n",
            "Epoch 10/10, Loss: 0.522296984253637\n",
            "Test Accuracy: 69.51%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69.51"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results (no Knowledge Distillation)\n",
        "\n",
        "### Teacher Model\n",
        "\n",
        "*   Number of parameters: 1,186,986\n",
        "*   Time to train: ~3 minutes\n",
        "*   Train loss (last epoch): 0.09178\n",
        "*   Test accuracy: **74.43%**\n",
        "\n",
        "### Student Model\n",
        "\n",
        "*   Number of parameters: 267,738\n",
        "*   Time to train: ~2 minutes\n",
        "*   Train loss (last epoch): 0.5223\n",
        "*   Test accuracy: **69.51%**"
      ],
      "metadata": {
        "id": "kqe7diq4dGM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train using Knowledge Distillation\n",
        "\n",
        "Now, our goal is to use the predictions from the Teacher model (teacher logits) to improve the performance of our Student model\n",
        "\n",
        "To do so, we use an additional loss function, called **KL-divergence**\n",
        "\n"
      ],
      "metadata": {
        "id": "Tu4_JyNDeWre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_kd(teacher, student, train_loader, epochs, lr, device, alpha=0.5, T=10):\n",
        "  ce_loss = nn.CrossEntropyLoss() # traditional cross-entropy loss\n",
        "  optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "  teacher.eval() # eval mode (no update)\n",
        "  student.train() # train mode\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      with torch.no_grad(): # no need to calculate gradient for teacher\n",
        "        teacher_logits = teacher(inputs)\n",
        "\n",
        "      student_logits = student(inputs)\n",
        "      ce_loss_value = ce_loss(student_logits, labels)\n",
        "\n",
        "      soft_targets = F.softmax(teacher_logits / T, dim=-1)\n",
        "      soft_prob = F.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "      kd_loss_value = F.kl_div(soft_prob, soft_targets.detach(), reduction='batchmean') * (T * T) # KL-divergence loss\n",
        "      loss = ce_loss_value + kd_loss_value # combined loss (CE + KL)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
      ],
      "metadata": {
        "id": "NgDonm6teVKQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student Model Training with Knowledge Distillation and Testing\n",
        "\n",
        "torch.manual_seed(42)\n",
        "epoch = 10\n",
        "lr = 0.001\n",
        "train_with_kd(teacher_model, student_model, train_loader, 10, 0.001, device)\n",
        "test(student_model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdkHuc7-ix5r",
        "outputId": "39196cfe-64f6-4e64-f9e8-34cf8eccc824"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 12.14585326943556\n",
            "Epoch 2/10, Loss: 10.29619171552341\n",
            "Epoch 3/10, Loss: 9.561673729011165\n",
            "Epoch 4/10, Loss: 8.949190325139428\n",
            "Epoch 5/10, Loss: 8.506584990664821\n",
            "Epoch 6/10, Loss: 8.1307039590138\n",
            "Epoch 7/10, Loss: 7.781981824304137\n",
            "Epoch 8/10, Loss: 7.4728119635520995\n",
            "Epoch 9/10, Loss: 7.217940778073753\n",
            "Epoch 10/10, Loss: 6.9759596583178585\n",
            "Test Accuracy: 71.38%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71.38"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "### Teacher Model\n",
        "\n",
        "*   Number of parameters: 1,186,986\n",
        "*   Time to train: ~3 minutes\n",
        "*   Test accuracy: **74.43%**\n",
        "\n",
        "### Student Model (Without Knowledge Distillation)\n",
        "\n",
        "*   Number of parameters: 267,738\n",
        "*   Time to train: ~2 minutes\n",
        "*   Test accuracy: **69.51%**\n",
        "\n",
        "### Student Model (With Knowledge Distillation)\n",
        "\n",
        "*   Number of parameters: 267,738\n",
        "*   Time to train: ~2 minutes\n",
        "*   Test accuracy: **71.38%**"
      ],
      "metadata": {
        "id": "agn6-yIvjHSr"
      }
    }
  ]
}